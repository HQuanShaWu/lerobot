import os
import re
import time
import cv2
import torch
import importlib
from typing import Union, List, Optional, Tuple

from transformers import AutoTokenizer, AutoModel
from transformers.generation import BaseStreamer

# =========================================================
# 1) Speed Monitor Streamer (TTFT / TPS)
# =========================================================
class SpeedMonitorStreamer(BaseStreamer):
    """
    ç”¨äºç›‘æ§ç”Ÿæˆé€Ÿåº¦çš„ Streamerï¼ˆç»Ÿè®¡ TTFT / TPSï¼‰ã€‚
    HuggingFace generate åœ¨æ¯æ­¥ç”Ÿæˆ token æ—¶ä¼šè°ƒç”¨ streamer.put(...)
    """
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.start_time = None
        self.first_token_time = None
        self.end_time = None
        self.token_count = 0

    def put(self, value):
        now = time.time()
        if self.start_time is None:
            self.start_time = now
        if self.token_count == 0:
            self.first_token_time = now

        self.token_count += 1
        self.end_time = now

    def end(self):
        pass

    def get_stats(self, prefill_start_time: float):
        if self.first_token_time is None:
            return None

        ttft = self.first_token_time - prefill_start_time
        decoding_duration = (self.end_time - self.first_token_time) if self.end_time else 0.0
        if decoding_duration > 0 and self.token_count > 1:
            tps = (self.token_count - 1) / decoding_duration
        else:
            tps = 0.0

        return {
            "token_count": self.token_count,
            "ttft_sec": ttft,
            "gen_duration_sec": decoding_duration,
            "tps": tps,
        }

# =========================================================
# 2) InternVL image preprocess (å®˜æ–¹ dynamic tiling æ€è·¯)
#    æ¥æºï¼šInternVL3 Quick Start / README çš„ç¤ºä¾‹é€»è¾‘
# =========================================================
from PIL import Image
import torchvision.transforms as T
from torchvision.transforms.functional import InterpolationMode

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

def build_transform(input_size: int):
    return T.Compose([
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),
    ])

def dynamic_preprocess(
    image: Image.Image,
    image_size: int = 448,
    use_thumbnail: bool = True,
    max_num: int = 12,
):
    """
    å°†ä»»æ„åˆ†è¾¨ç‡å›¾ç‰‡åˆ‡æˆè‹¥å¹² image_size x image_size çš„ tileï¼ˆæœ€å¤š max_num ä¸ªï¼‰ï¼Œå¯é¢å¤–åŠ ç¼©ç•¥å›¾ tileã€‚
    è¿™ä¸ InternVL3 å®˜æ–¹ quick start çš„ç¤ºä¾‹ä¸€è‡´ã€‚ :contentReference[oaicite:1]{index=1}
    """
    w, h = image.size
    aspect_ratio = w / h

    # é€‰æ‹©ä¸€ä¸ª tiles å¸ƒå±€ï¼ˆå°½é‡æ¥è¿‘åŸå§‹æ¯”ä¾‹ï¼‰ï¼Œä¸” tiles æ•°ä¸è¶…è¿‡ max_num
    # è¿™é‡Œç”¨ä¸€ä¸ªç®€å•ç­–ç•¥ï¼šæšä¸¾ grid (gw, gh) ä½¿ gw*gh<=max_numï¼Œä¸” gw/gh æ¥è¿‘ aspect_ratio
    best_gw, best_gh = 1, 1
    best_diff = 1e9
    for gh in range(1, max_num + 1):
        for gw in range(1, max_num + 1):
            if gw * gh > max_num:
                continue
            diff = abs((gw / gh) - aspect_ratio)
            if diff < best_diff:
                best_diff = diff
                best_gw, best_gh = gw, gh

    target_w = best_gw * image_size
    target_h = best_gh * image_size
    resized = image.resize((target_w, target_h), resample=Image.BICUBIC)

    tiles = []
    for j in range(best_gh):
        for i in range(best_gw):
            left = i * image_size
            upper = j * image_size
            right = left + image_size
            lower = upper + image_size
            tiles.append(resized.crop((left, upper, right, lower)))

    if use_thumbnail and len(tiles) != 1:
        tiles.append(image.resize((image_size, image_size), resample=Image.BICUBIC))

    return tiles

def load_image(image_file: str, input_size: int = 448, max_num: int = 12, use_thumbnail: bool = True):
    image = Image.open(image_file).convert("RGB")
    transform = build_transform(input_size=input_size)
    tiles = dynamic_preprocess(image, image_size=input_size, use_thumbnail=use_thumbnail, max_num=max_num)
    pixel_values = torch.stack([transform(t) for t in tiles])
    return pixel_values  # [n_tiles, 3, H, W]

# =========================================================
# 3) Unified Inference for InternVL3 (like RoboBrain script)
# =========================================================
class UnifiedInferenceInternVL3:
    """
    ä»¿ç…§ test_robobrain2.0-3B.py çš„åŠŸèƒ½ï¼š
    - inference(): æ¨ç† + TTFT/TPS
    - get_action_condition(): forward ä¸€æ¬¡æ‹¿æœ€åä¸€å±‚ hidden state çš„æœ€å token
    - plot: å¯¹ pointing/grounding/trajectory ç­‰ç”»å›¾
    """

    def __init__(
        self,
        model_dir: str,
        device_map: str = "auto",
        torch_dtype: Optional[torch.dtype] = None,
    ):
        print(f"Loading InternVL3 checkpoint from: {model_dir}")

        # dtypeï¼šé»˜è®¤ä¼˜å…ˆ bf16ï¼ˆå¤šæ•° A100/æ–°å¡å¯ç”¨ï¼‰ï¼Œå¦åˆ™ fp16
        if torch_dtype is None:
            if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8:
                torch_dtype = torch.bfloat16
            else:
                torch_dtype = torch.float16

        self.model_dir = model_dir
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, use_fast=False)

        # å®˜æ–¹æ¨è AutoModel + trust_remote_codeï¼Œå†ç”¨ model.chatã€‚ :contentReference[oaicite:2]{index=2}
        self.model = AutoModel.from_pretrained(
            model_dir,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            device_map=device_map,
        ).eval()

        # InternVL çš„ chat é‡Œç”¨åˆ°çš„ token å­—ç¬¦ä¸²ï¼ˆä¸å®˜æ–¹ modeling_internvl_chat.py ä¸€è‡´ï¼‰ :contentReference[oaicite:3]{index=3}
        self.IMG_START_TOKEN = "<img>"
        self.IMG_END_TOKEN = "</img>"
        self.IMG_CONTEXT_TOKEN = "<IMG_CONTEXT>"

        # è®¾å¤‡
        try:
            self.device = self.model.device
        except Exception:
            self.device = next(self.model.parameters()).device

        self.dtype = torch_dtype
        print(f"Running on device: {self.device}, dtype: {self.dtype}")

        # åŠ¨æ€å¯¼å…¥ conversation.get_conv_templateï¼ˆç”¨äº get_action_condition å¤åˆ» chat çš„ prompt æ„é€ ï¼‰
        self.get_conv_template = self._resolve_get_conv_template()

    def _resolve_get_conv_template(self):
        """
        ä» model çš„ remote_code æ¨¡å—é‡Œå®šä½ conversation.get_conv_template
        """
        model_mod = self.model.__class__.__module__            # e.g. transformers_modules.xxx.modeling_internvl_chat
        pkg = model_mod.rsplit(".", 1)[0]                      # e.g. transformers_modules.xxx
        conv_mod = importlib.import_module(pkg + ".conversation")
        return getattr(conv_mod, "get_conv_template")

    def _prepare_images(
        self,
        image: Union[str, List[str], None],
        input_size: int = 364,     # æ¯ä¸ª tile resize åˆ° input_size x input_size
        max_num: int = 6,          # æ¯å¼ å›¾æœ€å¤šåˆ‡æˆ max_num ä¸ª tileï¼ˆå¯èƒ½é¢å¤–+thumbnailï¼‰
        use_thumbnail: bool = True # æ˜¯å¦é¢å¤–åŠ å…¥ç¼©ç•¥å›¾ tileï¼ˆé€šå¸¸èƒ½æå‡é²æ£’æ€§ï¼‰
    ) -> Tuple[Optional[torch.Tensor], List[int]]:
        """
        è¿”å›:
        pixel_values: [sum_tiles, 3, H, W] or None
        num_patches_list: æ¯å¼ å›¾å„è‡ª tile æ•°ï¼Œç”¨äº multi-image çš„ chat
        """
        if image is None:
            return None, []

        if isinstance(image, str):
            image_list = [image]
        else:
            if not isinstance(image, list):
                raise TypeError(f"image must be str|list[str]|None, got {type(image)}")
            image_list = image
            if not all(isinstance(p, str) for p in image_list):
                raise TypeError("image list must be list[str] (each item is an image path)")

        pixel_list: List[torch.Tensor] = []
        num_patches_list: List[int] = []

        for p in image_list:
            pv = load_image(p, input_size=input_size, max_num=max_num, use_thumbnail=use_thumbnail)
            num_patches_list.append(int(pv.size(0)))
            pixel_list.append(pv)

        pixel_values = torch.cat(pixel_list, dim=0).contiguous()  # [sum_tiles, 3, H, W]
        pixel_values = pixel_values.to(device=self.device, dtype=self.dtype, non_blocking=True)
        return pixel_values, num_patches_list


    def inference(
        self,
        text: str,
        image: Union[str, List[str]],
        task: str = "general",
        plot: bool = False,
        do_sample: bool = True,
        temperature: float = 0.7,
        max_new_tokens: int = 768,
        input_size: int = 364,
        max_num: int = 6,
        use_thumbnail: bool = True,
    ):
        # ä»»åŠ¡ç±»å‹æ ¡éªŒ
        assert task in ["general", "static", "prediction", "grounding"], \
            f"Invalid task={task} (expected: general/static/prediction/grounding)"

        text = "" if text is None else str(text)

        # 1) å›¾ç‰‡ -> pixel_values / num_patches_listï¼ˆä½ åŸæ¥ä¸‹é¢ä¼šç”¨åˆ°å®ƒä¿©ï¼‰
        # input_size: tile çš„è¾¹é•¿ï¼›max_num: æœ€å¤šåˆ‡å¤šå°‘ tileï¼›use_thumbnail: æ˜¯å¦åŠ ç¼©ç•¥å›¾ tile
        pixel_values, num_patches_list = self._prepare_images(
            image=image,
            input_size=input_size,
            max_num=max_num,
            use_thumbnail=use_thumbnail,
        )

        # 2) æ„é€  task å¯¹åº”çš„æ–‡æœ¬ï¼ˆé‡ç‚¹ï¼šgrounding çš„ <ref>ï¼‰
        if task == "general":
            # long_captionï¼šå¦‚æœæ²¡ç»™ promptï¼Œå°±ç»™é»˜è®¤é•¿æè¿°æç¤º
            if not text.strip():
                text = "Please describe in detail the scene and the objects in the image."

        elif task == "grounding":
            # ç›®æ ‡æ ¼å¼ï¼ˆä½ å¾®è°ƒæ—¶ç”¨çš„ï¼‰ï¼š
            # Please provide the bounding box coordinate of the region this sentence describes: <ref>...</ref>
            instr = "Please provide the bounding box coordinate of the region this sentence describes"

            has_ref = ("<ref>" in text) and ("</ref>" in text)
            has_instr = (instr.lower() in text.lower())

            if has_instr and not has_ref:
                # ç”¨æˆ·å·²ç»ç»™äº†å®Œæ•´æŒ‡ä»¤ï¼Œä½†æ²¡ç»™ <ref>ï¼šå°½é‡æŠŠå†’å·åå†…å®¹åŒ…è¿› <ref>
                if ":" in text:
                    prefix, desc = text.split(":", 1)
                    desc = desc.strip().rstrip(".")
                    text = f"{prefix.strip()}: <ref>{desc}</ref>"
                else:
                    # æ²¡å†’å·å°±åªèƒ½æ•´ä½“å½“ä½œæè¿°ï¼ˆä¿å®ˆå¤„ç†ï¼‰
                    desc = text.strip().rstrip(".")
                    text = f"{instr}: <ref>{desc}</ref>"

            elif (not has_instr) and has_ref:
                # å·²ç»æ˜¯ <ref>...</ref>ï¼Œä½†æ²¡æœ‰æŒ‡ä»¤ï¼šè¡¥é½æŒ‡ä»¤
                # æ³¨æ„ï¼šä¸è¦åœ¨ </ref> åé¢é¢å¤–åŠ å¥å·ï¼Œé¿å…å½±å“ä½ å¯¹è¾“å‡ºæ ¼å¼çš„æœŸå¾…
                text = f"{instr}: {text.strip()}"

            elif (not has_instr) and (not has_ref):
                # åªæœ‰æè¿°è¯­å¥ï¼šè‡ªåŠ¨åŒ… <ref> å¹¶åŠ æŒ‡ä»¤
                desc = text.strip().rstrip(".")
                text = f"{instr}: <ref>{desc}</ref>"

            # else: has_instr and has_ref -> ç”¨æˆ·ç»™çš„å·²ç»å®Œå…¨ç¬¦åˆï¼Œä¸åŠ¨

        elif task == "static":
            # robovqa_staticï¼šé™æ€å›¾åƒé—®ç­”ï¼ˆå½“å‰å¸§/å½“å‰åœºæ™¯ï¼‰
            # å¦‚æœè°ƒç”¨æ–¹æ²¡ç»™ promptï¼Œå°±æä¾›ä¸€ä¸ªé»˜è®¤é—®é¢˜ï¼Œé¿å…ç©ºè¾“å…¥å¯¼è‡´æ— æ„ä¹‰è¾“å‡º
            if not text.strip():
                text = "What can the robot do immediately given the current scene?"

        elif task == "prediction":
            # robovqa_future_predictionï¼šæœªæ¥é¢„æµ‹ï¼ˆåŸºäºå½“å‰ç”»é¢æ¨æ–­ä¸‹ä¸€æ­¥/æœªæ¥çŠ¶æ€ï¼‰
            if not text.strip():
                text = "Please predict what will happen next in the scene."


        # 3) ç¡®ä¿ <image> å ä½ç¬¦æ•°é‡ä¸å›¾ç‰‡æ•°é‡ä¸€è‡´ï¼ˆInternVL æ ¼å¼è¦æ±‚ï¼‰
        n_images = len(num_patches_list)

        n_placeholders = text.count("<image>")

        if n_images == 0:
            if n_placeholders > 0:
                raise ValueError("Text-only input should not contain '<image>' placeholder.")
            question = text
        else:
            if n_placeholders == 0:
                # é»˜è®¤å‰ç¼€è¡¥é½ n_images ä¸ª <image>\n
                question = ("<image>\n" * n_images) + text
            else:
                if n_placeholders != n_images:
                    raise ValueError(
                        f"Number of '<image>' placeholders ({n_placeholders}) must match "
                        f"number of images ({n_images})."
                    )
                question = text

        print(f"\n{'='*20} INPUT {'='*20}\n{question}\n{'='*47}\n")

        # 4) speed monitorï¼ˆé€šè¿‡ generate çš„ streamer ç»Ÿè®¡ï¼‰
        speed_streamer = SpeedMonitorStreamer(self.tokenizer)
        generation_config = dict(
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            streamer=speed_streamer,
        )

        # 5) è®¡æ—¶ï¼šä¸åŸè„šæœ¬ä¸€æ ·ï¼Œåœ¨â€œå¼€å§‹ generate å‰â€æ‰“ç‚¹
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        t_start = time.time()

        # InternVL chatï¼šå¤šå›¾/å•å›¾éƒ½èµ° model.chat
        if num_patches_list:
            response = self.model.chat(
                self.tokenizer,
                pixel_values,
                question,
                generation_config,
                num_patches_list=num_patches_list,
            )
        else:
            response = self.model.chat(
                self.tokenizer,
                None,
                question,
                generation_config,
            )

        if torch.cuda.is_available():
            torch.cuda.synchronize()

        # 6) æ€§èƒ½ç»Ÿè®¡è¾“å‡º
        stats = speed_streamer.get_stats(prefill_start_time=t_start)
        print(f"\n{'='*10} Performance Stats {'='*10}")
        if stats:
            print(f"ğŸ“Š TTFT (é¦–å­—å»¶è¿Ÿ):     {stats['ttft_sec']:.3f} s")
            print(f"ğŸš€ Speed (ç”Ÿæˆé€Ÿåº¦):    {stats['tps']:.2f} tokens/s")
            print(f"ğŸ”¢ Total Tokens:        {stats['token_count']}")
        else:
            print("No tokens streamed (maybe streamer not triggered).")
        print(f"{'='*47}\n")

        # 7) ç”»å›¾ï¼šä½ çš„æ–°ä»»åŠ¡é‡Œåªæœ‰ grounding éœ€è¦ç”»æ¡†ï¼ˆå¦‚ä½ éœ€è¦ static/prediction ä¹Ÿç”»ï¼Œå†æ‰©å±•ï¼‰
        if plot and task == "grounding":
            img_path = image if isinstance(image, str) else image[0]
            self._handle_plotting(img_path, response, task)

        return {"answer": response}


    def get_action_condition(
        self,
        text: str,
        image: Union[str, List[str]],
        input_size: int = 364,
        max_num: int = 6,
        use_thumbnail: bool = True,
    ):
        """
        å¤åˆ» InternVL çš„ chat prompt æ„é€ ï¼Œç„¶å forward ä¸€æ¬¡æ‹¿ outputs.hidden_states[-1][:, -1, :]
        æ³¨æ„ï¼šInternVL çš„ forward é‡Œä¼šç”¨åˆ° img_context_token_id / image_flags ç­‰ï¼ˆè§å®˜æ–¹ modelingï¼‰ã€‚ :contentReference[oaicite:5]{index=5}
        """
        pixel_values, num_patches_list = self._prepare_images(
            image=image, input_size=input_size, max_num=max_num, use_thumbnail=use_thumbnail
        )

        # question ä¸ inference ä¿æŒä¸€è‡´
        if isinstance(image, str):
            question = f"<image>\n{text}"
        else:
            prefix = "".join([f"Image-{i+1}: <image>\n" for i in range(len(image))])
            question = prefix + text

        # ===== æŒ‰ modeling_internvl_chat.py çš„ chat é€»è¾‘æ„é€  query ===== :contentReference[oaicite:6]{index=6}
        if num_patches_list is None:
            num_patches_list = [pixel_values.shape[0]] if pixel_values is not None else []

        # è®¾ç½® img_context_token_idï¼ˆforward / generate éƒ½ä¼šç”¨åˆ°ï¼‰
        img_context_token_id = self.tokenizer.convert_tokens_to_ids(self.IMG_CONTEXT_TOKEN)
        self.model.img_context_token_id = img_context_token_id

        template = self.get_conv_template(self.model.template)
        template.system_message = self.model.system_message
        template.append_message(template.roles[0], question)
        template.append_message(template.roles[1], None)
        query = template.get_prompt()

        for n_patch in num_patches_list:
            image_tokens = (
                self.IMG_START_TOKEN +
                (self.IMG_CONTEXT_TOKEN * self.model.num_image_token * n_patch) +
                self.IMG_END_TOKEN
            )
            query = query.replace("<image>", image_tokens, 1)

        model_inputs = self.tokenizer(query, return_tensors="pt")
        input_ids = model_inputs["input_ids"].to(self.device)
        attention_mask = model_inputs["attention_mask"].to(self.device)

        # InternVL forward é‡Œ image_flags ä¼šå‚ä¸ç­› vit_embedsï¼ˆæœ‰äº›ç‰ˆæœ¬æ˜¯å¿…éœ€çš„ï¼‰ :contentReference[oaicite:7]{index=7}
        image_flags = None
        if pixel_values is not None:
            image_flags = torch.ones((pixel_values.shape[0], 1), dtype=torch.long, device=self.device)

        with torch.no_grad():
            outputs = self.model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                image_flags=image_flags,
                output_hidden_states=True,
                return_dict=True,
            )

        last_layer = outputs.hidden_states[-1]          # [B, seq, hidden]
        action_condition = last_layer[:, -1, :]         # [B, hidden]
        print(f"Action Condition Extracted. Shape: {action_condition.shape}")
        return action_condition


    # ---------------- plot utils ----------------
    # ä»…ç”¨äº grounding ä»»åŠ¡ï¼šåœ¨åŸå›¾ä¸Šç”»å‡ºé¢„æµ‹ bbox
    def _handle_plotting(self, image_path: str, result_text: str, task: str = "grounding"):
        if task != "grounding":
            # ä¿é™©ï¼šå³ä½¿ä¸Šå±‚è¯¯è°ƒç”¨ï¼Œä¹Ÿä¸åšä»»ä½•ç»˜åˆ¶
            print(f"Plot skipped (task={task}). Only 'grounding' is supported for plotting.")
            return None

        print("Plotting enabled (grounding). Drawing bounding boxes on the image ...")

        # æ”¯æŒè¾“å‡ºåŒ…å« <box>[[x1, y1, x2, y2]]</box> æˆ–ç›´æ¥ [[x1, y1, x2, y2]] çš„æƒ…å†µ
        box_pattern = r'\[\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\]'
        boxes = re.findall(box_pattern, result_text)

        if not boxes:
            print("No bounding box found in model output. Skip saving annotated image.")
            return None

        plot_boxes = [[int(x1), int(y1), int(x2), int(y2)] for x1, y1, x2, y2 in boxes]

        image_name = os.path.basename(image_path)
        name, ext = os.path.splitext(image_name)
        save_name = f"{name}_grounding_annotated{ext}"
        os.makedirs("result", exist_ok=True)
        save_path = os.path.join("result", save_name)

        return self.draw_on_image(image_path=image_path, boxes=plot_boxes, output_path=save_path)

    def draw_on_image(self, image_path: str, boxes: Optional[List[List[int]]] = None, output_path: Optional[str] = None):
        '''
        ä»…æ”¯æŒ groundingï¼šç”» bboxï¼ˆç»¿è‰²æ¡†ï¼‰ã€‚
        boxes: List[[x1, y1, x2, y2], ...]ï¼Œé»˜è®¤æŒ‰åƒç´ åæ ‡ç»˜åˆ¶ã€‚
        '''
        try:
            image = cv2.imread(image_path)
            if image is None:
                raise FileNotFoundError(f"Unable to read image: {image_path}")

            if boxes:
                for x1, y1, x2, y2 in boxes:
                    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)

            if output_path is None:
                os.makedirs("result", exist_ok=True)
                image_name = os.path.basename(image_path)
                name, ext = os.path.splitext(image_name)
                output_path = os.path.join("result", f"{name}_grounding_annotated{ext}")

            cv2.imwrite(output_path, image)
            print(f"Annotated image saved to: {output_path}")
            return output_path
        except Exception as e:
            print(f"Error processing image: {e}")
            return None


if __name__ == "__main__":
    MODEL_PATH = "/home/nvidia/internvl3_1b_sft"
    IMAGE_PATH = "/home/nvidia/embodied_debug_dump/long_caption/02_static_149852/img01_219439_aff.jpg"
    PROMPT = "What is shown in this image?"

    print("=== Initializing InternVL3 Model ===")
    bot = UnifiedInferenceInternVL3(MODEL_PATH)

    print("\n=== Test 1: Inference & Speed Test ===")
    result = bot.inference(
        text=PROMPT,
        image=IMAGE_PATH,
        task="general",
        plot=False,
        # ä¸‹é¢ä¸‰ä¸ªå‚æ•°å»ºè®®ä¸ä½ è®­ç»ƒ/æµ‹è¯•è®¾ç½®å¯¹é½ï¼š
        input_size=364,      # tile çš„è¾¹é•¿ï¼Œå¸¸è§ 364/448ï¼ˆä½ ä¹‹å‰ç”¨è¿‡ 364ï¼‰ :contentReference[oaicite:8]{index=8}
        max_num=6,           # æ¯å¼ å›¾æœ€å¤šåˆ‡å¤šå°‘å— tile
        use_thumbnail=True,  # æ˜¯å¦é¢å¤–åŠ ä¸€å¼ ç¼©ç•¥å›¾ tileï¼ˆå®˜æ–¹ç¤ºä¾‹ä¸º Trueï¼‰ :contentReference[oaicite:9]{index=9}
        max_new_tokens=256,  # ä½ æƒ³å‹æµ‹é€Ÿåº¦å¯æ”¹å¤§
    )
    print(f"Result: {result['answer']}")

    print("\n=== Test 2: Feature Extraction (last-layer hidden state) ===")
    condition = bot.get_action_condition(PROMPT, IMAGE_PATH, input_size=364, max_num=6, use_thumbnail=True)
    print("Condition extracted successfully.")
